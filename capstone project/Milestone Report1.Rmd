---
title: "Milestone Report 1: Exploratory analysis"
author: "Rachel Smith"
date: "3/16/2021"
output: html_document
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = TRUE, eval = FALSE, message = FALSE, warning = FALSE)


library(tidytext)
library(tidyverse)
library(wordcloud)
library(textdata)
library(igraph)
library(ggraph)

data(stop_words)

load("outputs/one_word_freqs.Rdata")
load("outputs/sentiment_plots.Rdata")
load("outputs/bi_tri_plots.Rdata")
load("outputs/n_unique_words.Rdata")

```

### Overview:

- This report contains exploratory analysis of the English text files for the Coursera Data Science Specialization through JHU
- The report contains:
        - analysis of frequencies of unigrams, bigrams, and trigrams (not including stop words)
        - sentiment analysis of unigrams using three different sentiment analysis datasets
        - assessment of how many words are needed to cover 50% and 90% of the words used in the English language
        - demonstration of a prediction algorithm using (a not math-heavy version of) Katz's back-off model
- I conclude by describing my future directions for creating the Shiny app

### Read in and format data
```{r data}


str_eng_news <- readLines("data/final/en_US/en_US.news.txt")
str_eng_blogs <- readLines("data/final/en_US/en_US.blogs.txt")
str_eng_twitter <- readLines("data/final/en_US/en_US.twitter.txt")


```

```{r format_data}

# convert each to tibble
df_eng_news <- tibble(line = 1:length(str_eng_news), text = str_eng_news)
df_eng_blogs <- tibble(line = 1:length(str_eng_blogs), text = str_eng_blogs)
df_eng_twitter <- tibble(line = 1:length(str_eng_twitter), text = str_eng_twitter)

# unnest tokens
df_eng_news_unnest <- df_eng_news %>% 
       unnest_tokens(word, text) %>% # makes lowercase
       mutate(source = "news")

df_eng_blogs_unnest <- df_eng_blogs %>% 
       unnest_tokens(word, text) %>% 
       mutate(source = "blogs")

df_eng_twitter_unnest <- df_eng_twitter %>% 
       unnest_tokens(word, text) %>% 
       mutate(source = "twitter")

# remove numbers since we're looking at words
df_eng_news_unnest <- df_eng_news_unnest %>% filter(!grepl("[0-9]", word))
df_eng_blogs_unnest <- df_eng_blogs_unnest %>% filter(!grepl("[0-9]", word))
df_eng_twitter_unnest <- df_eng_twitter_unnest %>% filter(!grepl("[0-9]", word))

# combine into one
df_eng <- rbind(df_eng_news_unnest, df_eng_blogs_unnest, df_eng_twitter_unnest)

# remove stop words
df_eng <- df_eng %>% anti_join(stop_words, by = "word")


```

### Determine most common single words across all sources
```{r one_word_freqs}

# distribution of words
df_eng_wordcount <- df_eng %>% count(word, sort = TRUE)     

df_eng_wordcount %>% head(n = 20) %>% mutate(word = reorder(word, n)) %>% 
       
       ggplot(aes(x = n, y = word)) +
       geom_col(fill = "#00BFC4") +
       theme_bw() +
       ggtitle("Most frequently used words in the English language")

```

```{r plot1, eval = TRUE, echo = FALSE}

p_eng_wordcount

```
Note: removed stop words for the purpose of looking at interesting word freqs, but will  want to include these in the text predictor.

#### Wordcloud
```{r wordcloud}

ggplot(df_eng_wordcount %>% head(n = 200), aes(label = word, size = n)) +
               geom_text_wordcloud_area(rm_outside = TRUE) +
               scale_size_area(max_size = 10) +
               theme_minimal()
```

```{r plot2, eval = TRUE, echo = FALSE}

p_eng_wordcloud

```

### Sentiment analysis
Get a feel for the sentiments being expressed by people in the news, on blogs, and on twitter, using three different sentiment analysis databases.

```{r sentiments}

df_afinn <- get_sentiments("afinn")
df_bing <- get_sentiments("bing")
df_nrc <- get_sentiments("nrc")

```

Overall (all sources combined)
```{r top_sentiments}

# AFINN
df_eng %>% 
       inner_join(df_afinn, by = "word") %>% 
       count(value, sort = TRUE) %>% 
       mutate(value = reorder(value, n)) %>%  
       
       ggplot(aes(x = n, y = as.factor(value))) +
       geom_col(fill = "#00BFC4") +
       ylab("value") +
       theme_bw() +
       ggtitle("Frequency of AFINN sentiments in English language")

# Bing et al.
df_eng %>% 
       inner_join(df_bing, by = "word") %>% 
       count(sentiment, sort = TRUE) %>% 
       mutate(sentiment = reorder(sentiment, n)) %>%  
       
       ggplot(aes(x = n, y = sentiment)) +
       geom_col(fill = "#00BFC4") +
       theme_bw() +
       ggtitle("Frequency of Bing et al. sentiments in English language")

# NRC
df_eng %>% 
       inner_join(df_nrc, by = "word") %>% 
       count(sentiment, sort = TRUE) %>% 
       mutate(sentiment = reorder(sentiment, n)) %>%  
       
       ggplot(aes(x = n, y = sentiment)) +
       geom_col(fill = "#00BFC4") +
       theme_bw() +
       ggtitle("Frequency of NRC sentiments in English language")

```

```{r plot3, echo = FALSE, eval = TRUE}

p_eng_afinn
p_eng_bing
p_eng_nrc

```

By source
```{r sent_by_source}

# AFINN
df_eng_afinn <- df_eng %>% inner_join(df_afinn, by = "word")
df_eng_afinn_count <- df_eng_afinn %>% count(value, source, sort = TRUE)
df_eng_afinn_count <- df_eng_afinn_count %>% 
       group_by(source) %>% 
       mutate(proportion = n/sum(n))

df_eng_afinn_count %>% 
       
       ggplot(aes(x = as.factor(value), y = proportion, fill = source)) +
       geom_col() +
       facet_wrap(~source) +
       
       theme_bw() +
       xlab("value") +
       ggtitle("AFINN sentiment analysis of each source") +
       theme(legend.position = "none")

# Bing
df_eng_bing <- df_eng %>% inner_join(df_bing, by = "word")
df_eng_bing_count <- df_eng_bing %>% count(sentiment, source, sort = TRUE)
df_eng_bing_count <- df_eng_bing_count %>% 
       group_by(source) %>% 
       mutate(proportion = n/sum(n))

df_eng_bing_count %>% 
       
       ggplot(aes(x = sentiment, y = proportion, fill = source)) +
       geom_col() +
       facet_wrap(~source) +
       
       theme_bw() +
       ggtitle("Bing et al. sentiment analysis of each source") +
       theme(legend.position = "none")

# NRC
df_eng_nrc <- df_eng %>% inner_join(df_nrc, by = "word")
df_eng_nrc_count <- df_eng_nrc %>% count(sentiment, source, sort = TRUE)
df_eng_nrc_count <- df_eng_nrc_count %>% 
       group_by(source) %>% 
       mutate(proportion = n/sum(n))

df_eng_nrc_count %>% 
       
       ggplot(aes(x = sentiment, y = proportion, fill = source)) +
       geom_col() +
       facet_wrap(~source) +
       
       theme_bw() +
       ggtitle("NRC sentiment analysis of each source") +
       theme(legend.position = "none",
             axis.text.x = element_text(angle = 45, hjust = 0.9))


```

```{r plot4, eval = TRUE, echo = FALSE}

p_eng_afinn_source
p_eng_bing_source
p_eng_nrc_source

```

Words contributing to each sentiment in each source
```{r sent_by_word}

# AFINN
df_eng_afinn_wordcounts <- df_eng_afinn %>% 
       count(word, value, sort = TRUE) %>% 
       mutate(sentiment = ifelse(value > 0, "positive", "negative"))

df_eng_afinn_wordcounts %>% 
       group_by(sentiment) %>% 
       slice_max(order_by = n, n = 10) %>% 
       ungroup() %>% 
       mutate(word = reorder(word, n)) %>% 
       
       ggplot(aes(x = n, y = word, fill = sentiment)) +
       geom_col(show.legend = FALSE) +
       facet_wrap(~sentiment, scales = "free_y") +
       theme_bw() +
       ggtitle("contribution of words to AFINN sentiment")

# Bing
df_eng_bing_wordcounts <- df_eng_bing %>% count(word, sentiment, sort = TRUE) 

df_eng_bing_wordcounts %>% 
       group_by(sentiment) %>% 
       slice_max(order_by = n, n = 10) %>% 
       ungroup() %>% 
       mutate(word = reorder(word, n)) %>% 
       
       ggplot(aes(x = n, y = word, fill = sentiment)) +
       geom_col(show.legend = FALSE) +
       facet_wrap(~sentiment, scales = "free_y") +
       theme_bw() +
       ggtitle("contribution of words to Bing et al. sentiment")

# NRC
df_eng_nrc_wordcounts <- df_eng_nrc %>% 
       count(word, sentiment, sort = TRUE) %>% 
       mutate(pos_neg = case_when(
              
              sentiment %in% c("positive", "trust", "joy", "surprise", "anticipation") ~ "positive",
              sentiment %in% c("anger", "disgust", "fear", "negative", "sadness") ~ "negative"
              
       ))

df_eng_nrc_wordcounts %>% 
       group_by(pos_neg) %>% 
       slice_max(order_by = n, n = 20) %>% 
       ungroup() %>% 
       
       ggplot(aes(x = n, y = word, fill = pos_neg)) +
       geom_col(show.legend = FALSE) +
       facet_wrap(~pos_neg, scales = "free_y") +
       theme_bw() +
       ggtitle("contribution of words to NRC sentiment")

```

```{r plot5, eval = TRUE, echo = FALSE}

p_eng_afinn_wordcounts
p_eng_bing_wordcounts
p_eng_nrc_wordcounts

```

### Bigrams
Sample 10% of data from each source because full files are too big for this analysis

```{r sample}

# sample from each 
df_eng_news_spl <- sample_n(df_eng_news, size = nrow(df_eng_news)*0.1)
df_eng_blogs_spl <- sample_n(df_eng_blogs, size = nrow(df_eng_blogs)*0.1)
df_eng_twitter_spl <- sample_n(df_eng_twitter, size = nrow(df_eng_twitter)*0.1)

# combine into one
df_eng_spl <- bind_rows(df_eng_news_spl, df_eng_blogs_spl, df_eng_twitter_spl)

```

```{r bigram}

# unnest to 2-gram
df_eng_bigram <- df_eng_spl %>% unnest_tokens(bigram, text, token = "ngrams", n = 2)

# remove lines with numbers 
df_eng_bigram <- df_eng_bigram %>% filter(!grepl("[0-9]", bigram))

# remove stop words
df_eng_bigrams_sep <- df_eng_bigram %>% separate(bigram, c("word1", "word2"), sep = " ")
df_eng_bigrams_filt <- df_eng_bigrams_sep %>%
       filter(!word1 %in% stop_words$word) %>%
       filter(!word2 %in% stop_words$word) %>% 
       filter(!is.na(word1), !is.na(word2))

# count
df_eng_bigrams_count <- df_eng_bigrams_filt %>% count(word1, word2, sort = TRUE)

# unite
df_eng_bigrams_count <- df_eng_bigrams_count %>% unite(bigram, word1, word2, sep = " ")

```

```{r plot_bigram}

# plot frequencies
df_eng_bigrams_count %>% head(n = 20) %>% mutate(bigram = reorder(bigram, n)) %>% 
               
               ggplot(aes(x = n, y = bigram)) +
               geom_col(fill = "#F8766D") +
               theme_bw() +
               ggtitle("Most frequent bigrams in the English language")

```

```{r plot6, eval = TRUE, echo = FALSE}

p_eng_bigrams_count

```

Visualize network of bigrams
```{r bigram_network}

igraph_eng_bigrams <- df_eng_bigrams_filt %>% 
       filter(!is.na(word1), !is.na(word2)) %>% 
       count(word1, word2, sort = TRUE) %>% 
       filter(n > 200) %>% 
       graph_from_data_frame()

```

```{r plot_network}

set.seed(2021)

ggraph(igraph_eng_bigrams, layout = "fr") +
       geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, end_cap = circle(.07, "inches")) +
       geom_node_point(color = "lightblue", size = 5) +
       geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
       theme_void()

```

```{r plot7, echo = FALSE, eval = TRUE}

p_bigram_network

```

### Trigrams

```{r format_trigram}

# unnest to 3-gram
df_eng_trigram <- df_eng_spl %>% unnest_tokens(trigram, text, token = "ngrams", n = 3)

# remove lines with numbers 
df_eng_trigram <- df_eng_trigram %>% filter(!grepl("[0-9]", trigram))

# remove stop words
df_eng_trigrams_sep <- df_eng_trigram %>% separate(trigram, c("word1", "word2", "word3"), sep = " ")
df_eng_trigrams_filt <- df_eng_trigrams_sep %>%
       filter(!word1 %in% stop_words$word) %>%
       filter(!word2 %in% stop_words$word) %>% 
       filter(!word3 %in% stop_words$word) %>% 
       filter(!is.na(word1), !is.na(word2), !is.na(word3))

# count
df_eng_trigrams_count <- df_eng_trigrams_filt %>% count(word1, word2, word3, sort = TRUE)

# unite
df_eng_trigrams_count <- df_eng_trigrams_count %>% unite(trigram, word1, word2, word3, sep = " ")

```

```{r plot_trigram}

df_eng_trigrams_count %>% head(n = 20) %>% mutate(trigram = reorder(trigram, n)) %>% 
       
       ggplot(aes(x = n, y = trigram)) +
       geom_col(fill = "#7CAE00") +
       theme_bw() +
       ggtitle("Most frequent trigrams in the English language")

```

```{r plot8, eval = TRUE, echo = FALSE}

p_eng_trigrams_count

```

### Unique words
Calculate the number of unique words in a frequency sorted dictionary needed to cover 50% and 90% of all word instances in the language.

```{r unique}

# include stop words this time
df_eng_all <- rbind(df_eng_news_unnest, df_eng_blogs_unnest, df_eng_twitter_unnest)

# frequency of all words including stop words
df_eng_wordcount_all <- df_eng_all %>% count(word, sort = TRUE)     

# loop through to calculate
n_total.words<- sum(df_eng_wordcount_all$n)
half <- 0.5*n_total.words
most <- 0.9*n_total.words

c <- 0
i <- 1 

while(c < half){
       c <- c + df_eng_wordcount_all[i,]$n;
       i <- i+1;
}

n_half <- i

while(c < most){
       c <- c + df_eng_wordcount_all[i,]$n;
       i <- i+1;
}

n_90 <- i

```

```{r 50_90, eval = TRUE}

n_half
n_90

```

### Prediction algorithm: Katz's back-off model
Using the same steps as for the bigrams and trigrams, I created a 4-gram dataframe. If a trigram exists as the first three words in a 4-gram, the following word prediction will be based off of the frequencies of the 4th word associated with those trigrams. If a trigram does not exist, the word prediction will instead use the bigram of the last two words to predict the third word. If this does not exist, it will use the unigram as the first word of a bigram.

```{r backoff}

# if trigram doesn't exist, back up to bigram
f_backoff_trigram <- function(word.1, word.2, word.3) {
        
        if (nrow(df_eng_4grams_count %>% filter(word1 == word.1, word2 == word.2, word3 == word.3)) > 0) {
                
                df <- df_eng_4grams_count %>% 
                        filter(word1 == word.1, word2 == word.2, word3 == word.3) %>% 
                        mutate(freq = n/sum(n))
                
        } else {
                
                df <- df_eng_trigrams_count %>% 
                        filter(word1 == word.2, word2 == word.3) %>% 
                        mutate(freq = n/sum(n))
                
        }
        
        return(df)
        
}

# if bigram doesn't exist, back up to unigram
f_backoff_bigram <- function(word.1, word.2) {
        
        if (nrow(df_eng_trigrams_count %>% filter(word1 == word.1, word2 == word.2)) > 0) {
                
                df <- df_eng_trigrams_count %>% 
                        filter(word1 == word.1, word2 == word.2) %>% 
                        mutate(freq = n/sum(n))
                
        } else {
                
                df <- df_eng_bigrams_count %>% 
                        filter(word1 == word.2) %>% 
                        mutate(freq = n/sum(n))
                
        }
        
        return(df)
        
}

# combine to back off
f_backoff <- function(word.1, word.2, word.3){
        
        
        if (nrow(f_backoff_trigram(word.1, word.2, word.3) > 0)) {
                
                df <- f_backoff_trigram(word.1, word.2, word.3)
                
        } else {
                
                df <- f_backoff_bigram(word.2, word.3)
                
        }
        
        return(df)
        
}

```

### Future directions

The Shiny App will contain a text bar where the user will input their text, then printed below will be the top three predictions based on frequency. I will use the above functions to determine frequency and therefore likelihood of different word combinations.






